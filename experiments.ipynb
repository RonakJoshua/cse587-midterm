{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.status.idle": "2025-02-28T06:26:27.522112Z",
     "shell.execute_reply": "2025-02-28T06:26:27.521646Z",
     "shell.execute_reply.started": "2025-02-28T06:26:25.173212Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.3)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.11.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (6.4.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "\n",
    "# !pip install --upgrade torch==2.1.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T06:26:27.523167Z",
     "iopub.status.busy": "2025-02-28T06:26:27.523003Z",
     "iopub.status.idle": "2025-02-28T06:26:33.065290Z",
     "shell.execute_reply": "2025-02-28T06:26:33.064892Z",
     "shell.execute_reply.started": "2025-02-28T06:26:27.523149Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 06:26:28.574818: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-28 06:26:28.574872: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-28 06:26:28.577968: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-28 06:26:28.588955: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-28 06:26:29.639019: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f6e3e1c3130>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T06:26:33.069411Z",
     "iopub.status.busy": "2025-02-28T06:26:33.069082Z",
     "iopub.status.idle": "2025-02-28T06:26:33.120887Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lets not forget that apple pay in 2014 require...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>here in nz 50 of retailers don’t even have con...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i will forever acknowledge this channel with t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>whenever i go to a place that doesn’t take app...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>apple pay is so convenient secure and easy to ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment Sentiment\n",
       "0  lets not forget that apple pay in 2014 require...   neutral\n",
       "1  here in nz 50 of retailers don’t even have con...  negative\n",
       "2  i will forever acknowledge this channel with t...  positive\n",
       "3  whenever i go to a place that doesn’t take app...  negative\n",
       "4  apple pay is so convenient secure and easy to ...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('kaggle_data/YoutubeCommentsDataSet.csv')\n",
    "\n",
    "# Check the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T06:26:33.121752Z",
     "iopub.status.busy": "2025-02-28T06:26:33.121580Z",
     "iopub.status.idle": "2025-02-28T06:26:33.135270Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "comment      0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "df = df.drop_duplicates()\n",
    "df = df.dropna()\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T06:26:33.135858Z",
     "iopub.status.busy": "2025-02-28T06:26:33.135717Z",
     "iopub.status.idle": "2025-02-28T06:26:33.252097Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1179/1308129695.py:13: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['sentiment'] = df['sentiment'].replace({'negative': 0, 'neutral': 1, 'positive': 2})\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = str(text).lower()  # Convert to lowercase\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove links\n",
    "    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
    "    text = re.sub(r'#\\w+', '', text)  # Remove hashtags\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters\n",
    "    return text.strip()\n",
    "\n",
    "df['cleaned_comment'] = df['comment'].apply(clean_text)\n",
    "df['word_count'] = df['cleaned_comment'].apply(lambda x: len(str(x).split()))\n",
    "df['char_count'] = df['cleaned_comment'].apply(lambda x: len(str(x)))\n",
    "\n",
    "df['sentiment'] = df['sentiment'].replace({'negative': 0, 'neutral': 1, 'positive': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T06:26:33.255353Z",
     "iopub.status.busy": "2025-02-28T06:26:33.252642Z",
     "iopub.status.idle": "2025-02-28T06:26:33.568072Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned_comment</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lets not forget that apple pay in 2014 require...</td>\n",
       "      <td>1</td>\n",
       "      <td>lets noot foorget that apple pay in rrequired ...</td>\n",
       "      <td>56</td>\n",
       "      <td>313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>here in nz 50 of retailers don’t even have con...</td>\n",
       "      <td>0</td>\n",
       "      <td>here in nz of retailers dont evenn havee connt...</td>\n",
       "      <td>28</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i will forever acknowledge this channel with t...</td>\n",
       "      <td>2</td>\n",
       "      <td>i willl foreveer acknowledge thhis channel wit...</td>\n",
       "      <td>31</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>whenever i go to a place that doesn’t take app...</td>\n",
       "      <td>0</td>\n",
       "      <td>whenever i go to a placee thhat doeesnt taake ...</td>\n",
       "      <td>83</td>\n",
       "      <td>441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>apple pay is so convenient secure and easy to ...</td>\n",
       "      <td>2</td>\n",
       "      <td>applee paay is so convvenient seecure andd eas...</td>\n",
       "      <td>26</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18403</th>\n",
       "      <td>i really like the point about engineering tool...</td>\n",
       "      <td>2</td>\n",
       "      <td>i rreally likke tthe pooint abouut engineeriin...</td>\n",
       "      <td>52</td>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18404</th>\n",
       "      <td>i’ve just started exploring this field and thi...</td>\n",
       "      <td>2</td>\n",
       "      <td>ive jusst starteed exploring this ffield and t...</td>\n",
       "      <td>23</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18405</th>\n",
       "      <td>excelente video con una pregunta filosófica pr...</td>\n",
       "      <td>1</td>\n",
       "      <td>exceelente video con una preguntaa fiilosfica ...</td>\n",
       "      <td>40</td>\n",
       "      <td>239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18406</th>\n",
       "      <td>hey daniel just discovered your channel a coup...</td>\n",
       "      <td>2</td>\n",
       "      <td>hhey daniiel just discoovered your channel a c...</td>\n",
       "      <td>16</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18407</th>\n",
       "      <td>this is great focus is key a playful approach ...</td>\n",
       "      <td>2</td>\n",
       "      <td>thiis is grreat focuus is key a pllayful apppr...</td>\n",
       "      <td>79</td>\n",
       "      <td>404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17874 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 comment  sentiment  \\\n",
       "0      lets not forget that apple pay in 2014 require...          1   \n",
       "1      here in nz 50 of retailers don’t even have con...          0   \n",
       "2      i will forever acknowledge this channel with t...          2   \n",
       "3      whenever i go to a place that doesn’t take app...          0   \n",
       "4      apple pay is so convenient secure and easy to ...          2   \n",
       "...                                                  ...        ...   \n",
       "18403  i really like the point about engineering tool...          2   \n",
       "18404  i’ve just started exploring this field and thi...          2   \n",
       "18405  excelente video con una pregunta filosófica pr...          1   \n",
       "18406  hey daniel just discovered your channel a coup...          2   \n",
       "18407  this is great focus is key a playful approach ...          2   \n",
       "\n",
       "                                         cleaned_comment  word_count  \\\n",
       "0      lets noot foorget that apple pay in rrequired ...          56   \n",
       "1      here in nz of retailers dont evenn havee connt...          28   \n",
       "2      i willl foreveer acknowledge thhis channel wit...          31   \n",
       "3      whenever i go to a placee thhat doeesnt taake ...          83   \n",
       "4      applee paay is so convvenient seecure andd eas...          26   \n",
       "...                                                  ...         ...   \n",
       "18403  i rreally likke tthe pooint abouut engineeriin...          52   \n",
       "18404  ive jusst starteed exploring this ffield and t...          23   \n",
       "18405  exceelente video con una preguntaa fiilosfica ...          40   \n",
       "18406  hhey daniiel just discoovered your channel a c...          16   \n",
       "18407  thiis is grreat focuus is key a pllayful apppr...          79   \n",
       "\n",
       "       char_count  \n",
       "0             313  \n",
       "1             159  \n",
       "2             183  \n",
       "3             441  \n",
       "4             135  \n",
       "...           ...  \n",
       "18403         276  \n",
       "18404         117  \n",
       "18405         239  \n",
       "18406          88  \n",
       "18407         404  \n",
       "\n",
       "[17874 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "def introduce_misspellings(text, typo_rate=0.5):\n",
    "    \"\"\"\n",
    "    Introduces realistic misspellings into a given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "        typo_rate (float): The probability of introducing a typo for each word (default: 0.1).\n",
    "\n",
    "    Returns:\n",
    "        str: Text with introduced misspellings.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    misspelled_words = []\n",
    "\n",
    "    keyboard_proximity = {  # Example keyboard layout (QWERTY)\n",
    "        'q': ['w', 'a', '1', '2'],\n",
    "        'w': ['q', 'e', 'a', 's', '2', '3'],\n",
    "        'e': ['w', 'r', 's', 'd', '3', '4'],\n",
    "        'r': ['e', 't', 'd', 'f', '4', '5'],\n",
    "        't': ['r', 'y', 'f', 'g', '5', '6'],\n",
    "        'y': ['t', 'u', 'g', 'h', '6', '7'],\n",
    "        'u': ['y', 'i', 'h', 'j', '7', '8'],\n",
    "        'i': ['u', 'o', 'j', 'k', '8', '9'],\n",
    "        'o': ['i', 'p', 'k', 'l', '9', '0'],\n",
    "        'p': ['o', '[', 'l', ';', '0', '-'],\n",
    "        'a': ['q', 'w', 's', 'z'],\n",
    "        's': ['a', 'w', 'e', 'd', 'z', 'x'],\n",
    "        'd': ['s', 'e', 'r', 'f', 'x', 'c'],\n",
    "        'f': ['d', 'r', 't', 'g', 'c', 'v'],\n",
    "        'g': ['f', 't', 'y', 'h', 'v', 'b'],\n",
    "        'h': ['g', 'y', 'u', 'j', 'b', 'n'],\n",
    "        'j': ['h', 'u', 'i', 'k', 'n', 'm'],\n",
    "        'k': ['j', 'i', 'o', 'l', 'm', ','],\n",
    "        'l': ['k', 'o', 'p', ';', ',', '.'],\n",
    "        'z': ['a', 's', 'x'],\n",
    "        'x': ['z', 's', 'd', 'c'],\n",
    "        'c': ['x', 'd', 'f', 'v'],\n",
    "        'v': ['c', 'f', 'g', 'b'],\n",
    "        'b': ['v', 'g', 'h', 'n'],\n",
    "        'n': ['b', 'h', 'j', 'm'],\n",
    "        'm': ['n', 'j', 'k', ','],\n",
    "        '1': ['q', '2'],\n",
    "        '2': ['1', 'q', 'w', '3'],\n",
    "        '3': ['2', 'w', 'e', '4'],\n",
    "        '4': ['3', 'e', 'r', '5'],\n",
    "        '5': ['4', 'r', 't', '6'],\n",
    "        '6': ['5', 't', 'y', '7'],\n",
    "        '7': ['6', 'y', 'u', '8'],\n",
    "        '8': ['7', 'u', 'i', '9'],\n",
    "        '9': ['8', 'i', 'o', '0'],\n",
    "        '0': ['9', 'o', 'p', '-'],\n",
    "        ',': ['m', 'k', 'l', '.'],\n",
    "        '.': [',', 'l', ';', '/'],\n",
    "        '?': ['.', '/'],\n",
    "        ' ': ['c', 'v', 'b', 'n', 'm'] # considering space as a key too for proximity errors\n",
    "    }\n",
    "    alphabet = list('abcdefghijklmnopqrstuvwxyz')\n",
    "\n",
    "    for word in words:\n",
    "        if random.random() < typo_rate: # Apply typo to word with probability typo_rate\n",
    "            if len(word) <= 2: # Avoid misspellings for very short words\n",
    "                misspelled_words.append(word)\n",
    "                continue\n",
    "\n",
    "            operations = ['keyboard_proximity', 'transpose', 'delete', 'duplicate_letter']\n",
    "            # operation_choice = random.choice(operations)\n",
    "            \n",
    "            operation_choice = 'duplicate_letter'\n",
    "\n",
    "            if operation_choice == 'keyboard_proximity':\n",
    "                index_to_replace = random.randint(0, len(word) - 1)\n",
    "                char_to_replace = word[index_to_replace].lower()\n",
    "                if char_to_replace in keyboard_proximity:\n",
    "                    possible_replacements = keyboard_proximity[char_to_replace]\n",
    "                    misspelled_char = random.choice(possible_replacements)\n",
    "                    if word[index_to_replace].isupper():\n",
    "                        misspelled_char = misspelled_char.upper()\n",
    "                    word_list = list(word)\n",
    "                    word_list[index_to_replace] = misspelled_char\n",
    "                    misspelled_words.append(\"\".join(word_list))\n",
    "\n",
    "\n",
    "            elif operation_choice == 'transpose': # Transposition (swap adjacent letters)\n",
    "                if len(word) >= 2:\n",
    "                    index_to_swap = random.randint(0, len(word) - 2)\n",
    "                    word_list = list(word)\n",
    "                    word_list[index_to_swap], word_list[index_to_swap + 1] = word_list[index_to_swap + 1], word_list[index_to_swap]\n",
    "                    misspelled_words.append(\"\".join(word_list))\n",
    "                else:\n",
    "                    misspelled_words.append(word) # Word too short for transposition\n",
    "\n",
    "\n",
    "            elif operation_choice == 'insert': # Insertion (add a letter)\n",
    "                index_to_insert = random.randint(0, len(word))\n",
    "                char_to_insert = random.choice(alphabet)\n",
    "                word_list = list(word)\n",
    "                word_list.insert(index_to_insert, char_to_insert)\n",
    "                misspelled_words.append(\"\".join(word_list))\n",
    "\n",
    "\n",
    "            elif operation_choice == 'delete': # Deletion (remove a letter)\n",
    "                index_to_delete = random.randint(0, len(word) - 1)\n",
    "                word_list = list(word)\n",
    "                del word_list[index_to_delete]\n",
    "                misspelled_words.append(\"\".join(word_list))\n",
    "\n",
    "            elif operation_choice == 'duplicate_letter': # Duplication (duplicate a letter)\n",
    "                index_to_duplicate = random.randint(0, len(word) - 1)\n",
    "                char_to_duplicate = word[index_to_duplicate]\n",
    "                word_list = list(word)\n",
    "                word_list.insert(index_to_duplicate, char_to_duplicate) #insert duplicate next to original\n",
    "                misspelled_words.append(\"\".join(word_list))\n",
    "\n",
    "            elif operation_choice == 'capitalize': # Capitalization error\n",
    "                 index_to_capitalize = random.randint(0, len(word) - 1)\n",
    "                 word_list = list(word)\n",
    "                 word_list[index_to_capitalize] = word_list[index_to_capitalize].upper() if word_list[index_to_capitalize].islower() else word_list[index_to_capitalize].lower() # Flip case\n",
    "                 misspelled_words.append(\"\".join(word_list))\n",
    "\n",
    "\n",
    "        else:\n",
    "            misspelled_words.append(word) # Keep original word if no typo introduced\n",
    "\n",
    "    return \" \".join(misspelled_words)\n",
    "\n",
    "# Example usage\n",
    "# text_data = \"This is an example of text data where we will introduce some realistic misspellings.\"\n",
    "# misspelled_text = introduce_misspellings(text_data, typo_rate=0.2) # 20% typo rate\n",
    "# print(\"Original text:\", text_data)\n",
    "# print(\"Misspelled text:\", misspelled_text)\n",
    "\n",
    "injected_typos = df.copy()\n",
    "injected_typos['cleaned_comment'] = df['cleaned_comment'].apply(introduce_misspellings)\n",
    "\n",
    "# df = pd.concat([df, injected_typos], ignore_index=True)\n",
    "\n",
    "# df = injected_typos\n",
    "\n",
    "# df.shape\n",
    "injected_typos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T06:26:33.568941Z",
     "iopub.status.busy": "2025-02-28T06:26:33.568773Z",
     "iopub.status.idle": "2025-02-28T06:26:53.211034Z"
    }
   },
   "outputs": [],
   "source": [
    "embeddings_model = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T06:26:53.212251Z",
     "iopub.status.busy": "2025-02-28T06:26:53.212065Z",
     "iopub.status.idle": "2025-02-28T06:26:53.217453Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_sentence_embedding(sentence, model):\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    embeddings = []\n",
    "    for token in tokens:\n",
    "        if token in model:\n",
    "            # print(token)\n",
    "            embeddings.append(model[token])\n",
    "        # Handling OOV words (options):\n",
    "        # 1. Skip OOV words (as in the original code):\n",
    "        else:\n",
    "            pass  # Or continue\n",
    "        # 2. Use a zero vector for OOV words:\n",
    "        # else:\n",
    "        #     embeddings.append(np.zeros(model.vector_size))\n",
    "        # 3. Use a special OOV vector (if you have one):\n",
    "        # else:\n",
    "        #     embeddings.append(model.wv[\"<OOV>\"])  # If you have an <OOV> token\n",
    "\n",
    "    if embeddings:\n",
    "        return np.array(embeddings)  # Average along the 0-axis (words)\n",
    "    else:\n",
    "        return np.array([np.zeros(model.vector_size)]) # Or return None if you prefer to filter later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T06:26:53.218675Z",
     "iopub.status.busy": "2025-02-28T06:26:53.218389Z",
     "iopub.status.idle": "2025-02-28T06:26:53.223333Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_dataset(df):\n",
    "    X_text = df['cleaned_comment']\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=300000) # Or adjust vocabulary size\n",
    "    tokenizer.fit_on_texts(X_text)\n",
    "    X_sequences = tokenizer.texts_to_sequences(X_text)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    len(X_sequences[0])\n",
    "\n",
    "    df['embeddings'] = df['cleaned_comment'].apply(lambda x: get_sentence_embedding(x, embeddings_model))\n",
    "\n",
    "    print(\"done\")\n",
    "\n",
    "    embeddings_list = []\n",
    "    max_len = 0 # Find the maximum sentence length\n",
    "\n",
    "    for emb in df['embeddings']:\n",
    "        embeddings_list.append(emb)\n",
    "        max_len = max(max_len, len(emb))\n",
    "\n",
    "    print(\"done\")\n",
    "\n",
    "\n",
    "\n",
    "    chunk_size = 500  # Example chunk size, you might need to experiment with this value\n",
    "\n",
    "    padded_chunks = []  # List to store padded chunks\n",
    "\n",
    "    # for i in range(0, len(embeddings_list), chunk_size):\n",
    "    #     # Extract a chunk of the embeddings_list\n",
    "    #     chunk = embeddings_list[i:i + chunk_size]\n",
    "\n",
    "    #     # Pad the current chunk\n",
    "    #     padded_chunk = pad_sequences(\n",
    "    #         chunk,\n",
    "    #         padding=\"post\",\n",
    "    #         maxlen=max_len,  # Keep maxlen the same as your original command\n",
    "    #         dtype='float32',\n",
    "    #         value=0.0\n",
    "    #     )\n",
    "    #     padded_chunks.append(padded_chunk) # Add the padded chunk to the list\n",
    "\n",
    "    #     # print(i)\n",
    "\n",
    "    # # Concatenate all padded chunks into a single numpy array\n",
    "    # embeddings_padded = np.concatenate(padded_chunks, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Pad the sequences so that they all have the same length\n",
    "    embeddings_padded = pad_sequences(embeddings_list, padding=\"post\", maxlen=max_len, dtype='float32', value = 0.0)\n",
    "    \n",
    "    return embeddings_padded\n",
    "\n",
    "#     print(\"done\")\n",
    "\n",
    "#     type(embeddings_padded)\n",
    "\n",
    "#     print(\"Embeddings Tensor:\")\n",
    "#     print(embeddings_padded[-1])\n",
    "#     print(\"Shape of Embeddings Tensor:\", embeddings_padded.shape)\n",
    "\n",
    "#     embeddings_tensor = torch.from_numpy(embeddings_padded)\n",
    "#     labels_tensor = torch.from_numpy(df['sentiment'].values)\n",
    "\n",
    "#     torch_dataset = TensorDataset(embeddings_tensor, labels_tensor)\n",
    "    \n",
    "#     return torch_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T06:26:53.223950Z",
     "iopub.status.busy": "2025-02-28T06:26:53.223780Z",
     "iopub.status.idle": "2025-02-28T06:27:17.169878Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 70694 unique tokens.\n",
      "done\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# embeddings_tensor = torch.from_numpy(build_dataset(df))\n",
    "# labels_tensor = torch.from_numpy(df['sentiment'].values)\n",
    "# torch_dataset = TensorDataset(embeddings_tensor, labels_tensor)\n",
    "\n",
    "embeddings_tensor = torch.from_numpy(build_dataset(injected_typos))\n",
    "labels_tensor = torch.from_numpy(injected_typos['sentiment'].values)\n",
    "torch_dataset = TensorDataset(embeddings_tensor, labels_tensor)\n",
    "\n",
    "save_path = './youtube_comments.pt'\n",
    "\n",
    "# Save the torch_dataset\n",
    "# torch.save(torch_dataset, save_path)\n",
    "\n",
    "# torch_dataset = build_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T06:27:17.171156Z",
     "iopub.status.busy": "2025-02-28T06:27:17.170937Z",
     "iopub.status.idle": "2025-02-28T06:27:17.176774Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "def create_dataloaders(dataset, train_ratio=0.8, batch_size=32):\n",
    "    \"\"\"Creates DataLoader objects for training and testing sets.\n",
    "\n",
    "    Args:\n",
    "        embeddings: A PyTorch tensor of shape (num_samples, seq_len, embedding_dim).\n",
    "        labels: A PyTorch tensor of shape (num_samples,).\n",
    "        train_ratio: The proportion of data to use for training.\n",
    "        batch_size: The batch size for the DataLoaders.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the training DataLoader and the testing DataLoader.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split into training and testing sets\n",
    "    train_size = int(len(dataset) * train_ratio)\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)  # Shuffle for training\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False) # No need to shuffle for testing.\n",
    "\n",
    "    return train_dataloader, test_dataloader\n",
    "\n",
    "\n",
    "train_dataloader, test_dataloader = create_dataloaders(torch_dataset, train_ratio=0.8, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T06:27:17.179619Z",
     "iopub.status.busy": "2025-02-28T06:27:17.179049Z",
     "iopub.status.idle": "2025-02-28T06:27:17.184892Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SentimentCNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_filters, kernel_sizes, num_classes):\n",
    "        super(SentimentCNN, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv_layers = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embedding_dim,\n",
    "                      out_channels=num_filters,\n",
    "                      kernel_size=ks)\n",
    "            for ks in kernel_sizes\n",
    "        ])\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(len(kernel_sizes) * num_filters, num_classes)\n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, embedding_dim)\n",
    "        # CNN expects input shape (batch_size, embedding_dim, seq_len)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        # Apply convolutional layers and pooling\n",
    "        pooled_outputs = []\n",
    "        for conv in self.conv_layers:\n",
    "            conved = F.relu(conv(x)) # Convolution and ReLU activation\n",
    "            pooled = F.max_pool1d(conved, conved.shape[2]).squeeze(2) # Max pooling over sequence length\n",
    "            pooled_outputs.append(pooled)\n",
    "\n",
    "        # Concatenate pooled outputs\n",
    "        concatenated = torch.cat(pooled_outputs, dim=1)\n",
    "        # Apply dropout\n",
    "        dropped = self.dropout(concatenated)\n",
    "        # Fully connected layer\n",
    "        output = self.fc(dropped)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T06:27:17.185635Z",
     "iopub.status.busy": "2025-02-28T06:27:17.185484Z",
     "iopub.status.idle": "2025-02-28T06:27:42.900367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.688951048951049\n",
      "Precision: 0.6489408252611937\n",
      "Recall: 0.6319116633122085\n",
      "F1-score: 0.6209166817452871\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.41      0.48       458\n",
      "           1       0.49      0.78      0.60       922\n",
      "           2       0.87      0.71      0.78      2195\n",
      "\n",
      "    accuracy                           0.69      3575\n",
      "   macro avg       0.65      0.63      0.62      3575\n",
      "weighted avg       0.74      0.69      0.70      3575\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_auc_score, #For binary classification only\n",
    ")\n",
    "# ... (Model loading and setup as before - load model, set to eval mode) ...\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = torch.load('epoch_typo_15.pth', map_location=device, weights_only=False) #map_location is important!\n",
    "model.to(device) #Move the model to the correct device.\n",
    "\n",
    "model.eval()  # VERY IMPORTANT: Sets the model to evaluation mode.  Crucial for correct inference.\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    \"\"\"Evaluates the model on the given dataloader and returns recall and precision.\"\"\"\n",
    "\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_embeddings, batch_labels in dataloader:\n",
    "            batch_embeddings = batch_embeddings.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "\n",
    "            outputs = model(batch_embeddings)\n",
    "            probabilities = F.softmax(outputs, dim=1)  # Get probabilities\n",
    "            predictions = torch.argmax(probabilities, dim=1)  # Get predicted labels\n",
    "\n",
    "            all_predictions.extend(predictions.cpu().numpy())  # Move to CPU for sklearn\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "            \n",
    "            # print('done batch')\n",
    "\n",
    "    # Calculate recall and precision using sklearn    \n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions, average='macro', zero_division=1) #Handles zero division\n",
    "    recall = recall_score(all_labels, all_predictions, average='macro', zero_division=1) #Handles zero division\n",
    "    f1 = f1_score(all_labels, all_predictions, average='macro', zero_division=1) #Handles zero division\n",
    "    conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "    class_report = classification_report(all_labels, all_predictions, zero_division=1) #Handles zero division\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1-score: {f1}\")\n",
    "    # print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "    print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "    return recall, precision\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "recall, precision = evaluate_model(model, test_dataloader, device)  # Replace test_dataloader with your test data's dataloader\n",
    "\n",
    "# print(f\"Recall: {recall}\")\n",
    "# print(f\"Precision: {precision}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
