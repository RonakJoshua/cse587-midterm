{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T21:58:52.624885Z",
     "iopub.status.busy": "2025-02-28T21:58:52.624269Z",
     "iopub.status.idle": "2025-02-28T21:58:57.160778Z",
     "shell.execute_reply": "2025-02-28T21:58:57.159907Z",
     "shell.execute_reply.started": "2025-02-28T21:58:52.624861Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.3)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.11.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (6.4.0)\n",
      "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: gensim\n",
      "Successfully installed gensim-4.3.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "\n",
    "# !pip install --upgrade torch==2.1.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T21:58:58.656309Z",
     "iopub.status.busy": "2025-02-28T21:58:58.655472Z",
     "iopub.status.idle": "2025-02-28T21:59:12.151367Z",
     "shell.execute_reply": "2025-02-28T21:59:12.150843Z",
     "shell.execute_reply.started": "2025-02-28T21:58:58.656285Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 21:59:01.964671: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-28 21:59:01.964777: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-28 21:59:02.112111: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-28 21:59:02.392083: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-28 21:59:04.115664: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f32f6e8f950>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T21:59:14.402760Z",
     "iopub.status.busy": "2025-02-28T21:59:14.402299Z",
     "iopub.status.idle": "2025-02-28T21:59:14.512968Z",
     "shell.execute_reply": "2025-02-28T21:59:14.512464Z",
     "shell.execute_reply.started": "2025-02-28T21:59:14.402743Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lets not forget that apple pay in 2014 require...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>here in nz 50 of retailers don’t even have con...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i will forever acknowledge this channel with t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>whenever i go to a place that doesn’t take app...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>apple pay is so convenient secure and easy to ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment Sentiment\n",
       "0  lets not forget that apple pay in 2014 require...   neutral\n",
       "1  here in nz 50 of retailers don’t even have con...  negative\n",
       "2  i will forever acknowledge this channel with t...  positive\n",
       "3  whenever i go to a place that doesn’t take app...  negative\n",
       "4  apple pay is so convenient secure and easy to ...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('kaggle_data/YoutubeCommentsDataSet.csv')\n",
    "\n",
    "# Check the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T21:59:17.564676Z",
     "iopub.status.busy": "2025-02-28T21:59:17.564460Z",
     "iopub.status.idle": "2025-02-28T21:59:17.601673Z",
     "shell.execute_reply": "2025-02-28T21:59:17.601281Z",
     "shell.execute_reply.started": "2025-02-28T21:59:17.564660Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "comment      0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "df = df.drop_duplicates()\n",
    "df = df.dropna()\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T21:59:20.382787Z",
     "iopub.status.busy": "2025-02-28T21:59:20.382571Z",
     "iopub.status.idle": "2025-02-28T21:59:20.499278Z",
     "shell.execute_reply": "2025-02-28T21:59:20.498856Z",
     "shell.execute_reply.started": "2025-02-28T21:59:20.382770Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_46/1308129695.py:13: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['sentiment'] = df['sentiment'].replace({'negative': 0, 'neutral': 1, 'positive': 2})\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = str(text).lower()  # Convert to lowercase\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove links\n",
    "    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
    "    text = re.sub(r'#\\w+', '', text)  # Remove hashtags\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters\n",
    "    return text.strip()\n",
    "\n",
    "df['cleaned_comment'] = df['comment'].apply(clean_text)\n",
    "df['word_count'] = df['cleaned_comment'].apply(lambda x: len(str(x).split()))\n",
    "df['char_count'] = df['cleaned_comment'].apply(lambda x: len(str(x)))\n",
    "\n",
    "df['sentiment'] = df['sentiment'].replace({'negative': 0, 'neutral': 1, 'positive': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T06:42:12.897843Z",
     "iopub.status.busy": "2025-02-28T06:42:12.897621Z",
     "iopub.status.idle": "2025-02-28T06:42:13.159174Z",
     "shell.execute_reply": "2025-02-28T06:42:13.158659Z",
     "shell.execute_reply.started": "2025-02-28T06:42:12.897821Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned_comment</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lets not forget that apple pay in 2014 require...</td>\n",
       "      <td>1</td>\n",
       "      <td>lets ot forget that apple pay in required a br...</td>\n",
       "      <td>56</td>\n",
       "      <td>313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>here in nz 50 of retailers don’t even have con...</td>\n",
       "      <td>0</td>\n",
       "      <td>here in nz of retailers dont even have contact...</td>\n",
       "      <td>28</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i will forever acknowledge this channel with t...</td>\n",
       "      <td>2</td>\n",
       "      <td>i will forever ackn9wledge this channel wth ht...</td>\n",
       "      <td>31</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>whenever i go to a place that doesn’t take app...</td>\n",
       "      <td>0</td>\n",
       "      <td>whenever i go to a place taht doesnt take appp...</td>\n",
       "      <td>83</td>\n",
       "      <td>441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>apple pay is so convenient secure and easy to ...</td>\n",
       "      <td>2</td>\n",
       "      <td>apple pay is so cnvenient secure wnd easy to u...</td>\n",
       "      <td>26</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18403</th>\n",
       "      <td>i really like the point about engineering tool...</td>\n",
       "      <td>2</td>\n",
       "      <td>i really like the point about engineering tool...</td>\n",
       "      <td>52</td>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18404</th>\n",
       "      <td>i’ve just started exploring this field and thi...</td>\n",
       "      <td>2</td>\n",
       "      <td>ive just started exploring this field amd this...</td>\n",
       "      <td>23</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18405</th>\n",
       "      <td>excelente video con una pregunta filosófica pr...</td>\n",
       "      <td>1</td>\n",
       "      <td>excelente video con una pregunta filosfcia prf...</td>\n",
       "      <td>40</td>\n",
       "      <td>239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18406</th>\n",
       "      <td>hey daniel just discovered your channel a coup...</td>\n",
       "      <td>2</td>\n",
       "      <td>hey daniel just discovered your channel a oupl...</td>\n",
       "      <td>16</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18407</th>\n",
       "      <td>this is great focus is key a playful approach ...</td>\n",
       "      <td>2</td>\n",
       "      <td>this is great focus is key a playfull approach...</td>\n",
       "      <td>79</td>\n",
       "      <td>404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17874 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 comment  sentiment  \\\n",
       "0      lets not forget that apple pay in 2014 require...          1   \n",
       "1      here in nz 50 of retailers don’t even have con...          0   \n",
       "2      i will forever acknowledge this channel with t...          2   \n",
       "3      whenever i go to a place that doesn’t take app...          0   \n",
       "4      apple pay is so convenient secure and easy to ...          2   \n",
       "...                                                  ...        ...   \n",
       "18403  i really like the point about engineering tool...          2   \n",
       "18404  i’ve just started exploring this field and thi...          2   \n",
       "18405  excelente video con una pregunta filosófica pr...          1   \n",
       "18406  hey daniel just discovered your channel a coup...          2   \n",
       "18407  this is great focus is key a playful approach ...          2   \n",
       "\n",
       "                                         cleaned_comment  word_count  \\\n",
       "0      lets ot forget that apple pay in required a br...          56   \n",
       "1      here in nz of retailers dont even have contact...          28   \n",
       "2      i will forever ackn9wledge this channel wth ht...          31   \n",
       "3      whenever i go to a place taht doesnt take appp...          83   \n",
       "4      apple pay is so cnvenient secure wnd easy to u...          26   \n",
       "...                                                  ...         ...   \n",
       "18403  i really like the point about engineering tool...          52   \n",
       "18404  ive just started exploring this field amd this...          23   \n",
       "18405  excelente video con una pregunta filosfcia prf...          40   \n",
       "18406  hey daniel just discovered your channel a oupl...          16   \n",
       "18407  this is great focus is key a playfull approach...          79   \n",
       "\n",
       "       char_count  \n",
       "0             313  \n",
       "1             159  \n",
       "2             183  \n",
       "3             441  \n",
       "4             135  \n",
       "...           ...  \n",
       "18403         276  \n",
       "18404         117  \n",
       "18405         239  \n",
       "18406          88  \n",
       "18407         404  \n",
       "\n",
       "[17874 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "def introduce_misspellings(text, typo_rate=0.2):\n",
    "    \"\"\"\n",
    "    Introduces realistic misspellings into a given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "        typo_rate (float): The probability of introducing a typo for each word (default: 0.1).\n",
    "\n",
    "    Returns:\n",
    "        str: Text with introduced misspellings.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    misspelled_words = []\n",
    "\n",
    "    keyboard_proximity = {  # Example keyboard layout (QWERTY)\n",
    "        'q': ['w', 'a', '1', '2'],\n",
    "        'w': ['q', 'e', 'a', 's', '2', '3'],\n",
    "        'e': ['w', 'r', 's', 'd', '3', '4'],\n",
    "        'r': ['e', 't', 'd', 'f', '4', '5'],\n",
    "        't': ['r', 'y', 'f', 'g', '5', '6'],\n",
    "        'y': ['t', 'u', 'g', 'h', '6', '7'],\n",
    "        'u': ['y', 'i', 'h', 'j', '7', '8'],\n",
    "        'i': ['u', 'o', 'j', 'k', '8', '9'],\n",
    "        'o': ['i', 'p', 'k', 'l', '9', '0'],\n",
    "        'p': ['o', '[', 'l', ';', '0', '-'],\n",
    "        'a': ['q', 'w', 's', 'z'],\n",
    "        's': ['a', 'w', 'e', 'd', 'z', 'x'],\n",
    "        'd': ['s', 'e', 'r', 'f', 'x', 'c'],\n",
    "        'f': ['d', 'r', 't', 'g', 'c', 'v'],\n",
    "        'g': ['f', 't', 'y', 'h', 'v', 'b'],\n",
    "        'h': ['g', 'y', 'u', 'j', 'b', 'n'],\n",
    "        'j': ['h', 'u', 'i', 'k', 'n', 'm'],\n",
    "        'k': ['j', 'i', 'o', 'l', 'm', ','],\n",
    "        'l': ['k', 'o', 'p', ';', ',', '.'],\n",
    "        'z': ['a', 's', 'x'],\n",
    "        'x': ['z', 's', 'd', 'c'],\n",
    "        'c': ['x', 'd', 'f', 'v'],\n",
    "        'v': ['c', 'f', 'g', 'b'],\n",
    "        'b': ['v', 'g', 'h', 'n'],\n",
    "        'n': ['b', 'h', 'j', 'm'],\n",
    "        'm': ['n', 'j', 'k', ','],\n",
    "        '1': ['q', '2'],\n",
    "        '2': ['1', 'q', 'w', '3'],\n",
    "        '3': ['2', 'w', 'e', '4'],\n",
    "        '4': ['3', 'e', 'r', '5'],\n",
    "        '5': ['4', 'r', 't', '6'],\n",
    "        '6': ['5', 't', 'y', '7'],\n",
    "        '7': ['6', 'y', 'u', '8'],\n",
    "        '8': ['7', 'u', 'i', '9'],\n",
    "        '9': ['8', 'i', 'o', '0'],\n",
    "        '0': ['9', 'o', 'p', '-'],\n",
    "        ',': ['m', 'k', 'l', '.'],\n",
    "        '.': [',', 'l', ';', '/'],\n",
    "        '?': ['.', '/'],\n",
    "        ' ': ['c', 'v', 'b', 'n', 'm'] # considering space as a key too for proximity errors\n",
    "    }\n",
    "    alphabet = list('abcdefghijklmnopqrstuvwxyz')\n",
    "\n",
    "    for word in words:\n",
    "        if random.random() < typo_rate: # Apply typo to word with probability typo_rate\n",
    "            if len(word) <= 2: # Avoid misspellings for very short words\n",
    "                misspelled_words.append(word)\n",
    "                continue\n",
    "\n",
    "            operations = ['keyboard_proximity', 'transpose', 'delete', 'duplicate_letter']\n",
    "            operation_choice = random.choice(operations)\n",
    "\n",
    "            if operation_choice == 'keyboard_proximity':\n",
    "                index_to_replace = random.randint(0, len(word) - 1)\n",
    "                char_to_replace = word[index_to_replace].lower()\n",
    "                if char_to_replace in keyboard_proximity:\n",
    "                    possible_replacements = keyboard_proximity[char_to_replace]\n",
    "                    misspelled_char = random.choice(possible_replacements)\n",
    "                    if word[index_to_replace].isupper():\n",
    "                        misspelled_char = misspelled_char.upper()\n",
    "                    word_list = list(word)\n",
    "                    word_list[index_to_replace] = misspelled_char\n",
    "                    misspelled_words.append(\"\".join(word_list))\n",
    "\n",
    "\n",
    "            elif operation_choice == 'transpose': # Transposition (swap adjacent letters)\n",
    "                if len(word) >= 2:\n",
    "                    index_to_swap = random.randint(0, len(word) - 2)\n",
    "                    word_list = list(word)\n",
    "                    word_list[index_to_swap], word_list[index_to_swap + 1] = word_list[index_to_swap + 1], word_list[index_to_swap]\n",
    "                    misspelled_words.append(\"\".join(word_list))\n",
    "                else:\n",
    "                    misspelled_words.append(word) # Word too short for transposition\n",
    "\n",
    "\n",
    "            elif operation_choice == 'insert': # Insertion (add a letter)\n",
    "                index_to_insert = random.randint(0, len(word))\n",
    "                char_to_insert = random.choice(alphabet)\n",
    "                word_list = list(word)\n",
    "                word_list.insert(index_to_insert, char_to_insert)\n",
    "                misspelled_words.append(\"\".join(word_list))\n",
    "\n",
    "\n",
    "            elif operation_choice == 'delete': # Deletion (remove a letter)\n",
    "                index_to_delete = random.randint(0, len(word) - 1)\n",
    "                word_list = list(word)\n",
    "                del word_list[index_to_delete]\n",
    "                misspelled_words.append(\"\".join(word_list))\n",
    "\n",
    "            elif operation_choice == 'duplicate_letter': # Duplication (duplicate a letter)\n",
    "                index_to_duplicate = random.randint(0, len(word) - 1)\n",
    "                char_to_duplicate = word[index_to_duplicate]\n",
    "                word_list = list(word)\n",
    "                word_list.insert(index_to_duplicate, char_to_duplicate) #insert duplicate next to original\n",
    "                misspelled_words.append(\"\".join(word_list))\n",
    "\n",
    "            elif operation_choice == 'capitalize': # Capitalization error\n",
    "                 index_to_capitalize = random.randint(0, len(word) - 1)\n",
    "                 word_list = list(word)\n",
    "                 word_list[index_to_capitalize] = word_list[index_to_capitalize].upper() if word_list[index_to_capitalize].islower() else word_list[index_to_capitalize].lower() # Flip case\n",
    "                 misspelled_words.append(\"\".join(word_list))\n",
    "\n",
    "\n",
    "        else:\n",
    "            misspelled_words.append(word) # Keep original word if no typo introduced\n",
    "\n",
    "    return \" \".join(misspelled_words)\n",
    "\n",
    "# Example usage\n",
    "# text_data = \"This is an example of text data where we will introduce some realistic misspellings.\"\n",
    "# misspelled_text = introduce_misspellings(text_data, typo_rate=0.2) # 20% typo rate\n",
    "# print(\"Original text:\", text_data)\n",
    "# print(\"Misspelled text:\", misspelled_text)\n",
    "\n",
    "injected_typos = df.copy()\n",
    "injected_typos['cleaned_comment'] = df['cleaned_comment'].apply(introduce_misspellings)\n",
    "\n",
    "# df = pd.concat([df, injected_typos], ignore_index=True)\n",
    "\n",
    "# df = injected_typos\n",
    "\n",
    "# df.shape\n",
    "injected_typos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T06:42:13.159844Z",
     "iopub.status.busy": "2025-02-28T06:42:13.159699Z",
     "iopub.status.idle": "2025-02-28T06:42:13.165529Z",
     "shell.execute_reply": "2025-02-28T06:42:13.164921Z",
     "shell.execute_reply.started": "2025-02-28T06:42:13.159829Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned_comment</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lets not forget that apple pay in 2014 require...</td>\n",
       "      <td>1</td>\n",
       "      <td>lets not forget that apple pay in  required a ...</td>\n",
       "      <td>56</td>\n",
       "      <td>313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>here in nz 50 of retailers don’t even have con...</td>\n",
       "      <td>0</td>\n",
       "      <td>here in nz  of retailers dont even have contac...</td>\n",
       "      <td>28</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i will forever acknowledge this channel with t...</td>\n",
       "      <td>2</td>\n",
       "      <td>i will forever acknowledge this channel with t...</td>\n",
       "      <td>31</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>whenever i go to a place that doesn’t take app...</td>\n",
       "      <td>0</td>\n",
       "      <td>whenever i go to a place that doesnt take appl...</td>\n",
       "      <td>83</td>\n",
       "      <td>441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>apple pay is so convenient secure and easy to ...</td>\n",
       "      <td>2</td>\n",
       "      <td>apple pay is so convenient secure and easy to ...</td>\n",
       "      <td>26</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  sentiment  \\\n",
       "0  lets not forget that apple pay in 2014 require...          1   \n",
       "1  here in nz 50 of retailers don’t even have con...          0   \n",
       "2  i will forever acknowledge this channel with t...          2   \n",
       "3  whenever i go to a place that doesn’t take app...          0   \n",
       "4  apple pay is so convenient secure and easy to ...          2   \n",
       "\n",
       "                                     cleaned_comment  word_count  char_count  \n",
       "0  lets not forget that apple pay in  required a ...          56         313  \n",
       "1  here in nz  of retailers dont even have contac...          28         159  \n",
       "2  i will forever acknowledge this channel with t...          31         183  \n",
       "3  whenever i go to a place that doesnt take appl...          83         441  \n",
       "4  apple pay is so convenient secure and easy to ...          26         135  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T06:42:13.166346Z",
     "iopub.status.busy": "2025-02-28T06:42:13.166189Z",
     "iopub.status.idle": "2025-02-28T06:42:13.169851Z",
     "shell.execute_reply": "2025-02-28T06:42:13.169288Z",
     "shell.execute_reply.started": "2025-02-28T06:42:13.166331Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1090"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['word_count'].max()\n",
    "\n",
    "# X_text = df['cleaned_comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T06:42:13.170552Z",
     "iopub.status.busy": "2025-02-28T06:42:13.170419Z",
     "iopub.status.idle": "2025-02-28T06:42:13.172766Z",
     "shell.execute_reply": "2025-02-28T06:42:13.172350Z",
     "shell.execute_reply.started": "2025-02-28T06:42:13.170540Z"
    }
   },
   "outputs": [],
   "source": [
    "# X_text = df['cleaned_comment']\n",
    "\n",
    "# tokenizer = Tokenizer(num_words=300000) # Or adjust vocabulary size\n",
    "# tokenizer.fit_on_texts(X_text)\n",
    "# X_sequences = tokenizer.texts_to_sequences(X_text)\n",
    "# word_index = tokenizer.word_index\n",
    "# print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "# len(X_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T21:59:25.977232Z",
     "iopub.status.busy": "2025-02-28T21:59:25.977021Z",
     "iopub.status.idle": "2025-02-28T21:59:45.353647Z",
     "shell.execute_reply": "2025-02-28T21:59:45.353181Z",
     "shell.execute_reply.started": "2025-02-28T21:59:25.977216Z"
    }
   },
   "outputs": [],
   "source": [
    "embeddings_model = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T21:59:55.806119Z",
     "iopub.status.busy": "2025-02-28T21:59:55.805481Z",
     "iopub.status.idle": "2025-02-28T21:59:55.809160Z",
     "shell.execute_reply": "2025-02-28T21:59:55.808779Z",
     "shell.execute_reply.started": "2025-02-28T21:59:55.806098Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_sentence_embedding(sentence, model):\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    embeddings = []\n",
    "    for token in tokens:\n",
    "        if token in model:\n",
    "            # print(token)\n",
    "            embeddings.append(model[token])\n",
    "        # Handling OOV words (options):\n",
    "        # 1. Skip OOV words (as in the original code):\n",
    "        else:\n",
    "            pass  # Or continue\n",
    "        # 2. Use a zero vector for OOV words:\n",
    "        # else:\n",
    "        #     embeddings.append(np.zeros(model.vector_size))\n",
    "        # 3. Use a special OOV vector (if you have one):\n",
    "        # else:\n",
    "        #     embeddings.append(model.wv[\"<OOV>\"])  # If you have an <OOV> token\n",
    "\n",
    "    if embeddings:\n",
    "        return np.array(embeddings)  # Average along the 0-axis (words)\n",
    "    else:\n",
    "        return np.array([np.zeros(model.vector_size)]) # Or return None if you prefer to filter later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T21:59:59.676915Z",
     "iopub.status.busy": "2025-02-28T21:59:59.676098Z",
     "iopub.status.idle": "2025-02-28T21:59:59.737712Z",
     "shell.execute_reply": "2025-02-28T21:59:59.737223Z",
     "shell.execute_reply.started": "2025-02-28T21:59:59.676886Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_dataset(df):\n",
    "    X_text = df['cleaned_comment']\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=300000) # Or adjust vocabulary size\n",
    "    tokenizer.fit_on_texts(X_text)\n",
    "    X_sequences = tokenizer.texts_to_sequences(X_text)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    len(X_sequences[0])\n",
    "\n",
    "    df['embeddings'] = df['cleaned_comment'].apply(lambda x: get_sentence_embedding(x, embeddings_model))\n",
    "\n",
    "    print(\"done\")\n",
    "\n",
    "    embeddings_list = []\n",
    "    max_len = 0 # Find the maximum sentence length\n",
    "\n",
    "    for emb in df['embeddings']:\n",
    "        embeddings_list.append(emb)\n",
    "        max_len = max(max_len, len(emb))\n",
    "\n",
    "    print(\"done\")\n",
    "\n",
    "\n",
    "\n",
    "    chunk_size = 500  # Example chunk size, you might need to experiment with this value\n",
    "\n",
    "    padded_chunks = []  # List to store padded chunks\n",
    "\n",
    "    # for i in range(0, len(embeddings_list), chunk_size):\n",
    "    #     # Extract a chunk of the embeddings_list\n",
    "    #     chunk = embeddings_list[i:i + chunk_size]\n",
    "\n",
    "    #     # Pad the current chunk\n",
    "    #     padded_chunk = pad_sequences(\n",
    "    #         chunk,\n",
    "    #         padding=\"post\",\n",
    "    #         maxlen=max_len,  # Keep maxlen the same as your original command\n",
    "    #         dtype='float32',\n",
    "    #         value=0.0\n",
    "    #     )\n",
    "    #     padded_chunks.append(padded_chunk) # Add the padded chunk to the list\n",
    "\n",
    "    #     # print(i)\n",
    "\n",
    "    # # Concatenate all padded chunks into a single numpy array\n",
    "    # embeddings_padded = np.concatenate(padded_chunks, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Pad the sequences so that they all have the same length\n",
    "    embeddings_padded = pad_sequences(embeddings_list, padding=\"post\", maxlen=max_len, dtype='float32', value = 0.0)\n",
    "    \n",
    "    return embeddings_padded\n",
    "\n",
    "#     print(\"done\")\n",
    "\n",
    "#     type(embeddings_padded)\n",
    "\n",
    "#     print(\"Embeddings Tensor:\")\n",
    "#     print(embeddings_padded[-1])\n",
    "#     print(\"Shape of Embeddings Tensor:\", embeddings_padded.shape)\n",
    "\n",
    "#     embeddings_tensor = torch.from_numpy(embeddings_padded)\n",
    "#     labels_tensor = torch.from_numpy(df['sentiment'].values)\n",
    "\n",
    "#     torch_dataset = TensorDataset(embeddings_tensor, labels_tensor)\n",
    "    \n",
    "#     return torch_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T22:00:02.902790Z",
     "iopub.status.busy": "2025-02-28T22:00:02.902246Z",
     "iopub.status.idle": "2025-02-28T22:01:21.180307Z",
     "shell.execute_reply": "2025-02-28T22:01:21.179602Z",
     "shell.execute_reply.started": "2025-02-28T22:00:02.902765Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 33366 unique tokens.\n",
      "done\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "embeddings_tensor = torch.from_numpy(build_dataset(df))\n",
    "labels_tensor = torch.from_numpy(df['sentiment'].values)\n",
    "torch_dataset = TensorDataset(embeddings_tensor, labels_tensor)\n",
    "\n",
    "# embeddings_tensor = torch.from_numpy(build_dataset(injected_typos))\n",
    "# labels_tensor = torch.from_numpy(injected_typos['sentiment'].values)\n",
    "# torch_dataset = TensorDataset(embeddings_tensor, labels_tensor)\n",
    "\n",
    "save_path = './youtube_comments.pt'\n",
    "\n",
    "# Save the torch_dataset\n",
    "# torch.save(torch_dataset, save_path)\n",
    "\n",
    "# torch_dataset = build_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# load_path = \"./\"\n",
    "# loaded_features = torch.load(os.path.join(load_path, \"dataset_typos.pt\"), weights_only=False)\n",
    "\n",
    "# # loaded_dataset = torch.utils.data.TensorDataset(loaded_features)\n",
    "\n",
    "# loaded_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T22:01:27.629468Z",
     "iopub.status.busy": "2025-02-28T22:01:27.629248Z",
     "iopub.status.idle": "2025-02-28T22:01:27.674437Z",
     "shell.execute_reply": "2025-02-28T22:01:27.673935Z",
     "shell.execute_reply.started": "2025-02-28T22:01:27.629450Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "def create_dataloaders(dataset, train_ratio=0.8, batch_size=32):\n",
    "    \"\"\"Creates DataLoader objects for training and testing sets.\n",
    "\n",
    "    Args:\n",
    "        embeddings: A PyTorch tensor of shape (num_samples, seq_len, embedding_dim).\n",
    "        labels: A PyTorch tensor of shape (num_samples,).\n",
    "        train_ratio: The proportion of data to use for training.\n",
    "        batch_size: The batch size for the DataLoaders.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the training DataLoader and the testing DataLoader.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split into training and testing sets\n",
    "    train_size = int(len(dataset) * train_ratio)\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size], )\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)  # Shuffle for training\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False) # No need to shuffle for testing.\n",
    "\n",
    "    return train_dataloader, test_dataloader\n",
    "\n",
    "\n",
    "train_dataloader, test_dataloader = create_dataloaders(torch_dataset, train_ratio=0.8, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T22:01:30.424274Z",
     "iopub.status.busy": "2025-02-28T22:01:30.424035Z",
     "iopub.status.idle": "2025-02-28T22:01:30.429498Z",
     "shell.execute_reply": "2025-02-28T22:01:30.429039Z",
     "shell.execute_reply.started": "2025-02-28T22:01:30.424257Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SentimentCNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_filters, kernel_sizes, num_classes):\n",
    "        super(SentimentCNN, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv_layers = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embedding_dim,\n",
    "                      out_channels=num_filters,\n",
    "                      kernel_size=ks)\n",
    "            for ks in kernel_sizes\n",
    "        ])\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(len(kernel_sizes) * num_filters, num_classes)\n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, embedding_dim)\n",
    "        # CNN expects input shape (batch_size, embedding_dim, seq_len)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        # Apply convolutional layers and pooling\n",
    "        pooled_outputs = []\n",
    "        for conv in self.conv_layers:\n",
    "            conved = F.relu(conv(x)) # Convolution and ReLU activation\n",
    "            pooled = F.max_pool1d(conved, conved.shape[2]).squeeze(2) # Max pooling over sequence length\n",
    "            pooled_outputs.append(pooled)\n",
    "\n",
    "        # Concatenate pooled outputs\n",
    "        concatenated = torch.cat(pooled_outputs, dim=1)\n",
    "        # Apply dropout\n",
    "        dropped = self.dropout(concatenated)\n",
    "        # Fully connected layer\n",
    "        output = self.fc(dropped)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T22:01:35.228355Z",
     "iopub.status.busy": "2025-02-28T22:01:35.227681Z",
     "iopub.status.idle": "2025-02-28T22:06:29.459067Z",
     "shell.execute_reply": "2025-02-28T22:06:29.458465Z",
     "shell.execute_reply.started": "2025-02-28T22:01:35.228305Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch [1/15]\n",
      "Epoch [1/15], Average Loss: 0.6215\n",
      "Start epoch [2/15]\n",
      "Epoch [2/15], Average Loss: 0.4912\n",
      "Start epoch [3/15]\n",
      "Epoch [3/15], Average Loss: 0.4131\n",
      "Start epoch [4/15]\n",
      "Epoch [4/15], Average Loss: 0.3539\n",
      "Start epoch [5/15]\n",
      "Epoch [5/15], Average Loss: 0.2893\n",
      "Start epoch [6/15]\n",
      "Epoch [6/15], Average Loss: 0.2314\n",
      "Start epoch [7/15]\n",
      "Epoch [7/15], Average Loss: 0.1952\n",
      "Start epoch [8/15]\n",
      "Epoch [8/15], Average Loss: 0.1621\n",
      "Start epoch [9/15]\n",
      "Epoch [9/15], Average Loss: 0.1411\n",
      "Start epoch [10/15]\n",
      "Epoch [10/15], Average Loss: 0.1264\n",
      "Start epoch [11/15]\n",
      "Epoch [11/15], Average Loss: 0.1112\n",
      "Start epoch [12/15]\n",
      "Epoch [12/15], Average Loss: 0.1040\n",
      "Start epoch [13/15]\n",
      "Epoch [13/15], Average Loss: 0.0987\n",
      "Start epoch [14/15]\n",
      "Epoch [14/15], Average Loss: 0.0856\n",
      "Start epoch [15/15]\n",
      "Epoch [15/15], Average Loss: 0.0881\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_dim = 300  # From your embedding tensor\n",
    "num_filters = 100    # Number of filters in CNN layers (you can experiment with this)\n",
    "kernel_sizes = [3, 4, 5] # Kernel sizes for CNN layers (trigram, 4-gram, 5-gram)\n",
    "num_classes = 3      # Number of sentiment classes (e.g., positive and negative)\n",
    "learning_rate = 0.001\n",
    "epochs = 15          # Number of training epochs\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# device = 'cpu'\n",
    "\n",
    "# Initialize the model\n",
    "model = SentimentCNN(embedding_dim, num_filters, kernel_sizes, num_classes).to(device)\n",
    "# model = torch.load('epoch_typo_10.pth', map_location=device, weights_only=False) #map_location is important!\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss() # For multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    print(f'Start epoch [{epoch+1}/{epochs}]')\n",
    "    model.train() # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    for batch_embeddings, batch_labels in train_dataloader:\n",
    "\n",
    "        batch_embeddings = batch_embeddings.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch_embeddings)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # print(f'{loss}')\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    # torch.save(model, f\"epoch_typo_{epoch+1}.pth\")\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T14:30:48.291743Z",
     "iopub.status.busy": "2025-02-28T14:30:48.291092Z",
     "iopub.status.idle": "2025-02-28T14:30:57.539782Z",
     "shell.execute_reply": "2025-02-28T14:30:57.539200Z",
     "shell.execute_reply.started": "2025-02-28T14:30:48.291722Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7966433566433566\n",
      "Precision: 0.7322796509908311\n",
      "Recall: 0.7464885507125637\n",
      "F1-score: 0.7389094240422626\n",
      "Confusion Matrix:\n",
      " [[ 310  101   47]\n",
      " [ 103  646  173]\n",
      " [  70  233 1892]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.68      0.66       458\n",
      "           1       0.66      0.70      0.68       922\n",
      "           2       0.90      0.86      0.88      2195\n",
      "\n",
      "    accuracy                           0.80      3575\n",
      "   macro avg       0.73      0.75      0.74      3575\n",
      "weighted avg       0.80      0.80      0.80      3575\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_auc_score, #For binary classification only\n",
    ")\n",
    "# ... (Model loading and setup as before - load model, set to eval mode) ...\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = torch.load('epoch_typo_15.pth', map_location=device, weights_only=False) #map_location is important!\n",
    "model.to(device) #Move the model to the correct device.\n",
    "\n",
    "model.eval()  # VERY IMPORTANT: Sets the model to evaluation mode.  Crucial for correct inference.\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    \"\"\"Evaluates the model on the given dataloader and returns recall and precision.\"\"\"\n",
    "\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_embeddings, batch_labels in dataloader:\n",
    "            batch_embeddings = batch_embeddings.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "\n",
    "            outputs = model(batch_embeddings)\n",
    "            probabilities = F.softmax(outputs, dim=1)  # Get probabilities\n",
    "            predictions = torch.argmax(probabilities, dim=1)  # Get predicted labels\n",
    "\n",
    "            all_predictions.extend(predictions.cpu().numpy())  # Move to CPU for sklearn\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "            \n",
    "            # print('done batch')\n",
    "\n",
    "    # Calculate recall and precision using sklearn    \n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions, average='macro', zero_division=1) #Handles zero division\n",
    "    recall = recall_score(all_labels, all_predictions, average='macro', zero_division=1) #Handles zero division\n",
    "    f1 = f1_score(all_labels, all_predictions, average='macro', zero_division=1) #Handles zero division\n",
    "    conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "    class_report = classification_report(all_labels, all_predictions, zero_division=1) #Handles zero division\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1-score: {f1}\")\n",
    "    print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "    print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "    return recall, precision\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "recall, precision = evaluate_model(model, test_dataloader, device)  # Replace test_dataloader with your test data's dataloader\n",
    "\n",
    "# print(f\"Recall: {recall}\")\n",
    "# print(f\"Precision: {precision}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
